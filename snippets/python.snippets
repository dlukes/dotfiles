snippet ifm "if main" b
if __name__ == "__main__":
	main($0)
endsnippet

snippet ifmd "if main with ipdb on exception" b
if __name__ == "__main__":
	with __import__("ipdb").launch_ipdb_on_exception():
		main($0)
endsnippet

snippet dbg "launch ipdb on exception"
with __import__("ipdb").launch_ipdb_on_exception():
	${VISUAL}
endsnippet

snippet gul "update globals with locals"
globals().update(locals())
endsnippet

snippet paracft "parallelism with concurrent.futures thread pool"
import time
import threading
import subprocess as sp
import concurrent.futures as cf

def worker(x):
	# A thread pool allows you to run either IO-bound Python bytecode
	# concurrently (which means the IO actions themselves will happen in
	# parallel), or CPU-bound external subprocesses in parallel.
	sp.run(["sleep", f"{x}s"])
	return threading.get_ident(), x

def log(tid, x):
	ts = str(int(time.time()))[-2:]
	print(f"Worker {tid}, job {x} done at ...{ts}.")

def main():
	# As of 3.8, the default for 'max_workers' with 'ThreadPoolExecutor'
	# is 'min(os.cpu_count() + 4, 32)', which is a three-way compromise
	# between using the pool for IO-bound tasks (where more workers than
	# CPUs is what you want, so that you can wait in parallel),
	# CPU-bound tasks (where you want roughly as many workers as you've
	# got CPUs), and not implicitly hogging all of the resources of
	# machines with many (> 32) cores.
	#
	# Still, if you have a more precise idea of the workload you're
	# submitting and the machine you'll be running it on, it's worth
	# tweaking this. E.g. if you want to paralellize subprocesses, which
	# is CPU-bound, something like 'int(os.cpu_count() * 0.9)' is
	# reasonable if you can afford to use more than 32 cores on big
	# machines, while still leaving a bit of headroom.
	with cf.ThreadPoolExecutor(max_workers=2) as ex:
		tasks = [5, 0, 4, 3, 2, 1]

		# The '.map()' method yields results as they come in and
		# preserves the ordering. This means that if the tasks have
		# uneven execution times, getting the results from quicker tasks
		# might be blocked behind slower tasks, as demonstrated by the
		# code below, which prints the first three results
		# simultaneously, once the first (longest-running) task is done:
		#
		#	Worker 139625394366208, job 5 done at ...90.
		#	Worker 139625385973504, job 0 done at ...90.
		#	Worker 139625385973504, job 4 done at ...90.
		#	Worker 139625385973504, job 3 done at ...92.
		#	Worker 139625394366208, job 2 done at ...92.
		#	Worker 139625394366208, job 1 done at ...93.
		#
		# Remember also that the 'chunksize' argument is ignored with
		# threads -- its purpose is to alleviate IPC cost, but there's
		# no IPC with threads, so it clearly makes no sense to chunk
		# tasks when using 'ThreadPoolExecutor'.
		#
		# Exceptions which occurred while performing a task are raised
		# at the point that its result should be yielded.
		for tid, x in ex.map(worker, tasks):
			log(tid, x)
		print()

		# Receiving results out of order, as soon as they're completed,
		# is slightly more complicated but possible. Note that
		# 'as_completed()' has the advantage of being able to juggle
		# futures driven to completion by different executors at the
		# same time. The output of the code below is the following:
		#
		#	Worker 139625377580800, job 0 done at ...88.
		#	Worker 139625377580800, job 4 done at ...92.
		#	Worker 139625385973504, job 5 done at ...93.
		#	Worker 139625385973504, job 2 done at ...95.
		#	Worker 139625377580800, job 3 done at ...95.
		#	Worker 139625385973504, job 1 done at ...96.
		for fut in cf.as_completed(ex.submit(worker, t) for t in tasks):
			# Accessing 'fut.result()' might raise an exception if
			# something went wrong while computing the future.
			tid, x = fut.result()
			log(tid, x)

		# Finally, if you'd like to process potentially unbounded
		# streams of data in real time, it looks like you'll have to use
		# queues and manage them manually, which can be a bit finicky:
		# https://stackoverflow.com/questions/41648103/how-would-i-go-about-using-concurrent-futures-and-queues-for-a-real-time-scenari
		#
		# See also https://bugs.python.org/issue29842.
		#
		# This is one case where you might want to consider using
		# 'multiprocessing' instead, which provides this ability out of
		# the box in its high-level API. That is, if you don't mind the
		# overhead associated with spawning multiple processes that you
		# don't technically need.

if __name__ == "__main__":
	main()
endsnippet

snippet paracfp "parallelism with concurrent.futures process pool"
import os
import time
import multiprocessing as mp
import concurrent.futures as cf

def worker(x):
	# A process pool allows you to run CPU-intensive Python bytecode in
	# parallel.
	time.sleep(x)
	return os.getpid(), x

def log(tid, x):
	ts = str(int(time.time()))[-2:]
	print(f"Worker {tid}, job {x} done at ...{ts}.")

def main():
	# The multiprocessing context allows you to specify how processes
	# are started:
	#
	# - "forkserver" is safe to mix with threading and fast, but only
	#	works on Unix platforms which support passing file descriptors
	#	over Unix pipes
	# - "spawn" is safe w.r.t. to threading but slow; it's the default
	#	on macOS and Windows
	# - "fork" is fast but can lead to problems with multithreaded code,
	#	cf. https://pythonspeed.com/articles/python-multiprocessing/;
	#	only available and default on Unix
	ctx = mp.get_context("forkserver")

	# The default for 'max_workers' with 'ProcessPoolExecutor' is
	# 'os.cpu_count()', because it expects CPU-bound tasks, but if you
	# want to make it easier for your computer to do other stuff at the
	# same time, then something like 'int(os.cpu_count() * 0.9)' seems
	# reasonable.
	with cf.ProcessPoolExecutor(max_workers=2, mp_context=ctx) as ex:
		tasks = [5, 0, 4, 3, 2, 1]

		# The '.map()' method yields results as they come in and
		# preserves the ordering. This means that if the tasks have
		# uneven execution times, getting the results from quicker tasks
		# might be blocked behind slower tasks, as demonstrated by the
		# code below, which prints the first three results
		# simultaneously, once the first (longest-running) task is done:
		#
		#	Worker 3264662, job 5 done at ...62.
		#	Worker 3264659, job 0 done at ...62.
		#	Worker 3264659, job 4 done at ...62.
		#	Worker 3264659, job 3 done at ...64.
		#	Worker 3264662, job 2 done at ...64.
		#	Worker 3264662, job 1 done at ...65.
		#
		# Beware of the 'chunksize' argument, it can be tricky. On the
		# surface, it seems simple: if I have 2 workers, 6 tasks and a
		# chunk size of 3, then the first 3 tasks will (probably) be
		# submitted to the first worker and the rest to the second,
		# right?
		#
		# Right, but since the purpose of 'chunksize' is to limit IPC
		# overhead, this also means that you *won't get any results
		# until an entire chunk is done running*. That's not a bug, it's
		# the whole raison d'etre for 'chunksize': instead of sending
		# tiny amounts of data between processes all the time, you send
		# larger amounts of data (= chunks) much less often. However,
		# that also means that if the chunks take a long time to
		# compute, you'll be stuck for a long time without any results
		# to pass on to downstream tasks or log (to make sure things are
		# running as expected), even though some tasks have already
		# completed in the child processes.
		#
		# So be careful with 'chunksize'. Rule of thumb: with
		# longer-running tasks and/or more data per task to send via
		# IPC, the default 'chunksize=1' is probably preferable,
		# especially if you want to log progress as results are
		# computed. With shorter-running tasks, especially with a small
		# amount of IPC data per task, it definitely makes sense to
		# chunk them to increase performance. When in doubt, benchmark.
		#
		# If you want all the gory details on why 'chunksize' is tricky
		# and parallel scheduling is hard, there's an excellent answer
		# over on SO: https://stackoverflow.com/a/54032744/1826241
		#
		# Exceptions which occurred while performing a task are raised
		# at the point that its result should be yielded.
		for tid, x in ex.map(worker, tasks):
			log(tid, x)
		print()

		# Receiving results out of order, as soon as they're completed,
		# is slightly more complicated but possible. Note that
		# 'as_completed()' has the advantage of being able to juggle
		# futures driven to completion by different executors at the
		# same time. The output of the code below is the following:
		#
		#	Worker 3264662, job 0 done at ...65.
		#	Worker 3264662, job 4 done at ...69.
		#	Worker 3264659, job 5 done at ...70.
		#	Worker 3264659, job 2 done at ...72.
		#	Worker 3264662, job 3 done at ...72.
		#	Worker 3264659, job 1 done at ...73.
		for fut in cf.as_completed(ex.submit(worker, t) for t in tasks):
			# Accessing 'fut.result()' might raise an exception if
			# something went wrong while computing the future.
			tid, x = fut.result()
			log(tid, x)

		# Finally, if you'd like to process potentially unbounded
		# streams of data in real time, it looks like you'll have to use
		# queues and manage them manually, which can be a bit finicky:
		# https://stackoverflow.com/questions/41648103/how-would-i-go-about-using-concurrent-futures-and-queues-for-a-real-time-scenari
		#
		# See also https://bugs.python.org/issue29842.
		#
		# This is one case where you might want to consider using
		# 'multiprocessing' instead, which provides this ability out of
		# the box in its high-level API.


if __name__ == "__main__":
	main()
endsnippet

snippet paramp "parallelism with multiprocessing"
import os
import time
import random
import multiprocessing as mp

# Cf. the 'ProcessPoolExecutor' snippet for details, this just shows how
# to achieve equivalent behavior by directly using the 'multiprocessing'
# module. 'concurrent.futures' is newer and it makes it easier to switch
# between thread- and process-based parallelism, so it's probably better
# to use that by default, but it's useful to have a reminder of how the
# features of the two modules map onto each other, because it can be a
# bit tricky.
#
# Also, some use cases are simpler with 'multiprocessing', e.g. handling
# unbounded / real-time streams as they come in is easily done with
# '.imap()' or '.imap_unordered()' (see below) because they're lazy,
# whereas with 'concurrent.futures', you'd have to resort to a more
# complicated setup involving manual management of queues.

def worker(x):
	time.sleep(x)
	return os.getpid(), x

def log(tid, x):
	ts = str(int(time.time()))[-2:]
	print(f"Worker {tid}, job {x} done at ...{ts}.")

def main():
	ctx = mp.get_context("forkserver")
	with ctx.Pool(processes=2) as pool:
		tasks = [5, 0, 4, 3, 2, 1]

		# Beware! Unlike in 'concurrent.futures', the '.map()' method
		# waits for all of the results to be done *and only then* yields
		# them, all at the same time. Sort of like the builtin 'map()'
		# in Python 2.x.
		#
		#	Worker 3266146, job 5 done at ...65.
		#	Worker 3266147, job 0 done at ...65.
		#	Worker 3266147, job 4 done at ...65.
		#	Worker 3266147, job 3 done at ...65.
		#	Worker 3266146, job 2 done at ...65.
		#	Worker 3266146, job 1 done at ...65.
		for pid, x in pool.map(worker, tasks):
			log(pid, x)
		print()

		# The counterpart of '.map()' from 'concurrent.futures' is
		# '.imap()', which yields results as they're available but
		# preserves ordering, which means results from quicker tasks can
		# be blocked behind slower ones.
		#
		#	Worker 3266147, job 5 done at ...70.
		#	Worker 3266146, job 0 done at ...70.
		#	Worker 3266146, job 4 done at ...70.
		#	Worker 3266146, job 3 done at ...72.
		#	Worker 3266147, job 2 done at ...72.
		#	Worker 3266147, job 1 done at ...73.
		for pid, x in pool.imap(worker, tasks):
			log(pid, x)
		print()

		# To get results immediately in the order that they become
		# available, use '.imap_unordered()'. In 'concurrent.futures',
		# this is not expressed with a '.map()' method variant, but with
		# the 'as_completed()' function.
		#
		#	Worker 3266147, job 0 done at ...73.
		#	Worker 3266147, job 4 done at ...77.
		#	Worker 3266146, job 5 done at ...78.
		#	Worker 3266146, job 2 done at ...80.
		#	Worker 3266147, job 3 done at ...80.
		#	Worker 3266146, job 1 done at ...81.
		for pid, x in pool.imap_unordered(worker, tasks):
			log(pid, x)
		print()

		# As with 'ProcessPoolExecutor' in 'concurrent.futures', note
		# that "getting results immediately" might exhibit unintuitive
		# behavior in practice: both '.imap()' and '.imap_unordered()'
		# accept 'chunksize' arguments, which means all of the caveats
		# about this tricky parameter apply here as well. In particular,
		# it means that a task's result is yielded not as soon as the
		# task is done, but only once the entire chunk that the task is
		# part of is done.

		# And finally, the one thing that requires a much more involved
		# setup with 'concurrent.futures', as mentioned above:
		# processing (potentially) unbounded streams in real time:
		def infinite():
			while True:
				yield random.randint(0, 3)

		for pid, x in pool.imap_unordered(worker, infinite()):
			log(pid, x)

if __name__ == "__main__":
	main()
endsnippet

snippet paracfq "unbounded parallelism with concurrent.futures and queues"
import time
import random
import threading
import subprocess as sp
import concurrent.futures as cf
from queue import Queue

# One possible way of handling unbounded streams of data in real time
# with 'ThreadPoolExecutor' and 'Queue'. You might also want to try and
# split the work into chunks on the main thread as suggested here:
# https://bugs.python.org/issue34168#msg322075
#
# That's probably easier to read and less tricky to get right, but the
# performance might be worse if you end up in a situation where some of
# the tasks in the chunk take a long time to complete while the rest of
# your workers sit idle, because no more work can come in until the
# chunk is done.

# The size of the Queue can be tricky to get right -- you need to apply
# some backpressure, but if your system is distributed, too much of it
# might mean that requests to this component start timing out.
QUEUE = Queue(10)

# Note that the producer should run in a separate thread, *not* a
# separate process, because that would create a copy of the queue
# instead of sharing it and the main process wouldn't be able to read
# from it. So if you're running your consumers in a
# 'ProcessPoolExecutor', you still need to have a 'ThreadPoolExecutor'
# to run the producer in.
def producer():
	while True:
		QUEUE.put(random.randint(0, 3))

def consumer(x):
	sp.run(["sleep", f"{x}s"])
	return threading.get_ident(), x

def log(tid, job_id, x):
	ts = str(int(time.time()))[-2:]
	print(f"Worker {tid}, job {job_id} (sleep {x}s) done at ...{ts}.")

def main():
	# Remember to set aside a worker for your producer! So if you want
	# 2 consumer workers, you need 3 workers total.
	with cf.ThreadPoolExecutor(max_workers=3) as ex:
		future2job_id = {}
		future2job_id[ex.submit(producer)] = None

		job_id = 0
		while future2job_id:
			done, _ = cf.wait(
				future2job_id, timeout=0.25, return_when=cf.FIRST_COMPLETED
			)

			# This condition is probably too simplistic -- in theory,
			# this while-loop could keep switching back and forth with
			# the producer thread, so the queue would never be empty and
			# tasks would just keep being submitted to the thread pool
			# and never being retrieved.
			while not QUEUE.empty():
				t = QUEUE.get()
				future2job_id[ex.submit(consumer, t)] = job_id
				job_id += 1

			for fut in done:
				jid = future2job_id[fut]
				del future2job_id[fut]
				if jid is not None:
					tid, x = fut.result()
					log(tid, jid, x)

if __name__ == "__main__":
	main()
endsnippet

snippet paratrio "parallelism with trio"
import time
import trio

# 'trio' is an alternative async runtime for Python. That means that
# like threads in Python, it can be used to parallelize IO-bound tasks
# or CPU-bound tasks run in subprocesses, but not CPU-bound Python code.
#
# It has tons of great ideas, some of which the stdlib async runtime
# 'asyncio' has been trying to incorporate of late. Its docs are a great
# resource (highly informative yet accessible) to learn about the
# pitfalls of async, its nuts and bolts, and how to do it right (or at
# least, less wrong): https://trio.readthedocs.io/
#
# In addition to the 'trio' docs, I cannot recommend enough the
# following articles on concurrency by the library's original creator:
#
# - https://vorpus.org/blog/notes-on-structured-concurrency-or-go-statement-considered-harmful/
# - https://vorpus.org/blog/timeouts-and-cancellation-for-humans/
# - https://vorpus.org/blog/control-c-handling-in-python-and-trio/
#
# The example below is very lightly adapted from
# https://trio.readthedocs.io/en/stable/reference-core.html#managing-multiple-producers-and-or-multiple-consumers

async def producer(producer_send_channel, tasks):
	# 'async with' makes sure that the other end of the channel gets
	# properly notified when this end gets closed.
	async with producer_send_channel:
		# 'tasks' could easily be an unbounded stream of data to process
		# on the fly.
		for x in tasks:
			await producer_send_channel.send(x)

async def consumer(worker_id, consumer_receive_channel, consumer_send_channel):
	async with consumer_receive_channel, consumer_send_channel:
		async for x in consumer_receive_channel:
			await trio.run_process(["sleep", f"{x}s"])
			await consumer_send_channel.send((worker_id, x))

async def main():
	num_workers = 2
	tasks = [5, 0, 4, 3, 2, 1]

	# Nurseries allow you to run a tree of child tasks concurrently.
	async with trio.open_nursery() as nursery:
		producer_send_channel, consumer_receive_channel = trio.open_memory_channel(0)
		consumer_send_channel, main_receive_channel = trio.open_memory_channel(0)

		nursery.start_soon(producer, producer_send_channel, tasks)

		async with consumer_receive_channel, consumer_send_channel:
			for worker_id in range(num_workers):
				nursery.start_soon(
					consumer,
					worker_id,
					# By cloning, we let each party have their own
					# handle on the channel, and only when all handles
					# are closed does the channel close and notify the
					# other end.
					consumer_receive_channel.clone(),
					consumer_send_channel.clone(),
				)

		async with main_receive_channel:
			async for worker_id, x in main_receive_channel:
				ts = str(int(time.time()))[-2:]
				print(f"Worker {worker_id} slept for {x}s, woke up at ...{ts}.")

if __name__ == "__main__":
	trio.run(main)
endsnippet

snippet fifo "fifo + subprocess"
import os
import tempfile
import subprocess as sp

# mktemp is currently considered unsafe as it returns a child of /tmp on
# Unix, so the path might be hijacked by an attacker (/tmp is world
# writable and the name is too short to be unguessable in practice).
# Though if hijacked by a malicious file, mkfifo will fail, so the worst
# that can happen is a crash (because we don't handle the failure, e.g.
# retry with a different name). A partial mitigation is to first create
# a private temp dir, reducing the risk of collision to processes run by
# the same/super user.
fifo_path = tempfile.mktemp()
os.mkfifo(fifo_path, 0o600)
try:
	# Opening a FIFO for writing blocks until someone else opens it for
	# reading, so we can't open for writing first or we'd block forever.
	# Instead, we first launch the subprocess which will read from the
	# FIFO. We have to do it in a non-blocking manner though, so that we
	# can go on to actually write to the FIFO, so no sp.run.
	proc = sp.Popen(["cat", fifo_path], stdout=sp.PIPE, text=True)
	with open(fifo_path, "w") as fifo:
		for c in "Kočka leze dírou, pes oknem.":
			print(c, file=fifo)
	proc.wait()
finally:
	os.unlink(fifo_path)

for l in proc.stdout:
	print(l.strip())
endsnippet

snippet richlog "logging with rich"
import logging
from rich.console import Console
from rich.logging import RichHandler

logging.basicConfig(
	level=logging.NOTSET,
	format="%(message)s",
	handlers=[
		RichHandler(console=Console(stderr=True), rich_tracebacks=True, markup=True)
	],
)

log = logging.getLogger("rich")
log.info("Starting up...")
try:
	print(1 / 0)
except Exception:
	log.exception("Unable to print!")
log.error("[bold red blink]Server is shutting down![/]")

# markup=True can be problematic according to the docs, so it can also
# be enabled per-message (which doesn't make it not problematic, it just
# limits the potential problems to that message):
# log.error("[bold red blink]Server is shutting down![/]", extra={"markup": True})
endsnippet

snippet richbar "monitoring progress with rich"
import time
from rich.console import Console
from rich.progress import track

console = Console()
console.log("[bold yellow]Started.[/]")
for _ in track(list(range(5)), description="Running..."):
	time.sleep(1)
console.log("[bold green blink]Done.[/]")
endsnippet

# vi: noet ts=4
